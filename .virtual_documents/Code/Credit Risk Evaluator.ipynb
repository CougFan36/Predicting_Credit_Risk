import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt


train_df = pd.read_csv(Path('../Resources/Generator/2019loans.csv'))
test_df = pd.read_csv(Path('../Resources/Generator/2020Q1loans.csv'))





#Review train data
train_df.head()


print(train_df.shape)


#Check for nulls in train data
if train_df.isnull().sum().any() > 0:
    print(train_df.isnull.sum())
else:
    print("No nulls in train data")


#Review test data
test_df.head()


print(train_df.shape)


#Check for nulls in test data
if train_df.isnull().sum().any() > 0:
    print(train_df.isnull.sum())
else:
    print("No nulls in test data")


# Convert categorical data to numeric and separate target feature for training data
train_dum_df = train_df.drop('target', axis=1)
train_dum_df = pd.get_dummies(train_dum_df)


train_dum_df.head()


# Convert categorical data to numeric and separate target feature for testing data
test_dum_df = test_df.drop('target', axis=1)
test_dum_df = pd.get_dummies(test_dum_df)

test_dum_df.head()


#Find missing dummy variable
train_cols = train_dum_df.columns.tolist()
test_cols = test_dum_df.columns.tolist()
missing = list(set(train_cols).difference(test_cols))

missing


# add missing dummy variables to testing set
test_dum_df['debt_settlement_flag_Y'] = False


# Train the Logistic Regression model on the unscaled data and print the model score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression

#Convert target label to 0 and 1
train_y_label = LabelEncoder().fit_transform(train_df['target'])
test_y_label = LabelEncoder().fit_transform(test_df['target'])

#Create logistic regression model (MODEL)
classifier = LogisticRegression()

#Fit the the model using training data
classifier.fit(train_dum_df,train_y_label)

print(f"Training Data Score: {classifier.score(train_dum_df, train_y_label)}")
print(f"Testing Data Score: {classifier.score(test_dum_df, test_y_label)}")


# Train a K-Nearest Neighbors model and print the model score
from sklearn.neighbors import KNeighborsClassifier
# Loop through different k values to find which has the highest accuracy.
# Note: We use only odd numbers because we don't want any ties.
train_scores = []
test_scores = []
for k in range(1, 20, 2):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(train_dum_df, train_y_label)
    train_score = knn.score(train_dum_df, train_y_label)
    test_score = knn.score(test_dum_df, test_y_label)
    train_scores.append(train_score)
    test_scores.append(test_score)
    print(f"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}")
    
    
plt.plot(range(1, 20, 2), train_scores, marker='o')
plt.plot(range(1, 20, 2), test_scores, marker="x")
plt.xlabel("k neighbors")
plt.ylabel("Testing accuracy Score")
plt.show()


#Based on the data above, there doesn't appear to be a standout number of clusters to use in the model.  However,
# 9 clusters appears to provide the highest test score where the train score begins to level off, so we will use that to fit the model.  
knn = KNeighborsClassifier(n_neighbors=9)
knn.fit(train_dum_df, train_y_label)
print('k=9 Test Acc: %.3f' % knn.score(test_dum_df, test_y_label))





# Scale and transform the data
from sklearn.preprocessing import StandardScaler

scaled_train = StandardScaler().fit(train_dum_df)

X_train_scaled = scaled_train.transform(train_dum_df)
X_test_scaled = scaled_train.transform(test_dum_df)


# Train the Logistic Regression model on the scaled data and print the model score

#Create logistic regression model (MODEL)
classifier_scaled = LogisticRegression()

#Fit the the model using training data (FIT)
classifier_scaled.fit(X_train_scaled,train_y_label)

print(f"Training Data Score: {classifier_scaled.score(X_train_scaled, train_y_label)}")
print(f"Testing Data Score: {classifier_scaled.score(X_test_scaled, test_y_label)}")


# Train a K-Nearest Neighbors model on the scaled data and print the model score

# Loop through different k values to find which has the highest accuracy.
# Note: We use only odd numbers because we don't want any ties.
scaled_train_scores = []
scaled_test_scores = []
for k in range(1, 20, 2):
    knn_scaled = KNeighborsClassifier(n_neighbors=k)
    knn_scaled.fit(X_train_scaled, train_y_label)
    scaled_train_score = knn_scaled.score(X_train_scaled, train_y_label)
    scaled_test_score = knn_scaled.score(X_test_scaled, test_y_label)
    scaled_train_scores.append(scaled_train_score)
    scaled_test_scores.append(scaled_test_score)
    print(f"k: {k}, Train/Test Score: {scaled_train_score:.3f}/{scaled_test_score:.3f}")
    
    
plt.plot(range(1, 20, 2), scaled_train_scores, marker='o')
plt.plot(range(1, 20, 2), scaled_test_scores, marker="x")
plt.xlabel("k neighbors")
plt.ylabel("Testing accuracy Score (Scaled Data)")
plt.show()


#Based on the data above, 9 clusters appears to be the best predictor for the scaled data before the accuracy starts levelling off, 
# so we will use 9 clusters for our predictor.  
knn_scaled = KNeighborsClassifier(n_neighbors=9)
knn_scaled.fit(X_train_scaled, train_y_label)
print('k=9 Test Acc: %.3f' % knn_scaled.score(X_test_scaled, test_y_label))


#Compare logistic regression and KNN test (predictor) accuracy scores from scaled data
print('Logistic Regression Accuracy: %.3f' % classifier_scaled.score(X_test_scaled, test_y_label))
print('kNN Clustering Accuracy: %.3f' % knn_scaled.score(X_test_scaled, test_y_label))


#Confusion matrix of scaled data (logistic regression)
from sklearn.metrics import confusion_matrix

y_true = test_y_label
y_pred = classifier_scaled.predict(X_test_scaled)
confusion_matrix(y_true, y_pred)


#False negative score
fn = (542)/(1764+542+587+1809)
fn






